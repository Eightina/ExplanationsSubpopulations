{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjJkOZuQ_t-o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "def add_scaler(clf, all_col_indices, cat_col_indices = []):\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', clf)\n",
        "    ]\n",
        "    if len(cat_col_indices) > 0:\n",
        "        numeric_features=list(set(all_col_indices)-set(cat_col_indices))\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, cat_col_indices)])\n",
        "\n",
        "        steps=[\n",
        "               ('scaler', preprocessor),\n",
        "               ('clf', clf)\n",
        "            ]\n",
        "\n",
        "    return Pipeline(steps)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "metric = 'accuracy' # accuracy, F1\n",
        "sens_attr = 'male' # white, male\n",
        "bbox = 'nn' # ['xgb', 'nn', 'lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6xMu70D_yd-"
      },
      "outputs": [],
      "source": [
        "class Parameter:\n",
        "  def __init__(self, name, val = None, range = None):\n",
        "    self.name = name\n",
        "    self.val = val\n",
        "    self.range = range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Hc2wBDAPyn"
      },
      "outputs": [],
      "source": [
        "p_correct = [[Parameter('P(User = Correct | BB = Incorrect, Explanation = None)', 0.6967821782), \n",
        "              Parameter('P(User = Correct | BB = Incorrect, Explanation = Poor)', 0.68), \n",
        "              Parameter('P(User = Correct | BB = Incorrect, Explanation = Good)', 0.6497395833)\n",
        "             ],\n",
        "             [Parameter('P(User = Correct | BB = Correct, Explanation = None)', 0.9250353607), \n",
        "              Parameter('P(User = Correct | BB = Correct, Explanation = Poor)', 0.9), \n",
        "              Parameter('P(User = Correct | BB = Correct, Explanation = Good)', 0.9280753968)\n",
        "              ]\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2PVoFeMBIvv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../lib/datasets/lsac.csv')\n",
        "\n",
        "train_cols = ['age',\n",
        "        'decile1',\n",
        "        'decile3',\n",
        "        'fam_inc',\n",
        "        'lsat',\n",
        "        'ugpa',\n",
        "        'cluster',\n",
        "        'fulltime']\n",
        "\n",
        "categorical_columns = ['cluster', 'fulltime']\n",
        "\n",
        "df['white'] = df['race1'].apply(lambda x: 1 if x == 0 else 0)\n",
        "\n",
        "X_train, X_test = train_test_split(df[train_cols + ['pass_bar', sens_attr]], test_size = 0.5, random_state = 42)\n",
        "y_train, y_test = X_train['pass_bar'], X_test['pass_bar']\n",
        "g_train, g_test = X_train[sens_attr], X_test[sens_attr]\n",
        "X_train = X_train.drop(columns =  ['pass_bar', sens_attr])\n",
        "X_test = X_test.drop(columns =  ['pass_bar', sens_attr])\n",
        "\n",
        "for g in df[sens_attr].unique():\n",
        "  print(f'{sens_attr} == {g}: percent = {(df[sens_attr] == g).sum()/len(df)}, target prevalence = {df[(df[sens_attr] == g)][\"pass_bar\"].value_counts(normalize= True)[1.0]} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYDmY6igFo8u"
      },
      "outputs": [],
      "source": [
        "def ohe_to_df(enc, col, name):\n",
        "    return pd.DataFrame(enc.transform(col.values.reshape(-1, 1)),\n",
        "        columns = [f'{name}=={i}'for i in enc.categories_[0]],\n",
        "        index = col.index)\n",
        "\n",
        "for col in categorical_columns:\n",
        "    X_train[col] = X_train[col].fillna(-1)\n",
        "    X_test[col] = X_test[col].fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4r7n8qDHIZy"
      },
      "outputs": [],
      "source": [
        "cat_col_indices = [list(X_train.columns).index(i) for i in categorical_columns]\n",
        "\n",
        "if bbox == 'xgb':\n",
        "  assert NotImplementedError\n",
        "  model = GridSearchCV(XGBClassifier(), param_grid = {'max_depth': list(range(6))}, cv = 5, scoring = 'roc_auc').fit(X_train, y_train)\n",
        "elif bbox == 'nn':\n",
        "  model = GridSearchCV(add_scaler(MLPClassifier(), \n",
        "        cat_col_indices = cat_col_indices, \n",
        "        all_col_indices=list(range(X_train.shape[1]))), param_grid = {'clf__hidden_layer_sizes': [(n_hidden, ) for n_hidden in [50, 100, 200]]},\n",
        "        cv = 5, scoring = 'roc_auc').fit(X_train, y_train)\n",
        "elif bbox == 'lr':\n",
        "  model = GridSearchCV(add_scaler(LogisticRegression(solver = 'liblinear'), \n",
        "        cat_col_indices = cat_col_indices, \n",
        "        all_col_indices=list(range(X_train.shape[1]))), param_grid = {'clf__C': 10.**np.linspace(-5, 1, 25)},\n",
        "        cv = 5, scoring = 'roc_auc').fit(X_train, y_train)\n",
        "else:\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV9vKPNQH746"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "for g in df[sens_attr].unique():\n",
        "  mask = g_test == g\n",
        "  print(f'% correct | {sens_attr} == {g}: {accuracy_score(y_test[mask], y_pred[mask])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1otknMLI4sS"
      },
      "outputs": [],
      "source": [
        "fidelity_mean = Parameter('Mean P(Explanation = Good)', 0.85)\n",
        "fidelity_diff = Parameter('P(Explanation = Good) Gap', 0.15, range = np.linspace(0, 0.15, 30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tQ3c5ONAG8T"
      },
      "outputs": [],
      "source": [
        "def get_metric(metric_name, user_correct, y):\n",
        "  if metric_name == 'accuracy':\n",
        "    return user_correct.sum()/len(user_correct)\n",
        "  elif metric_name == 'F1':\n",
        "    y_pred = np.where(user_correct == 1, y, ~(y == 1.0))\n",
        "    return f1_score(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qf2TT9tJtWa"
      },
      "outputs": [],
      "source": [
        "n_iters = 20\n",
        "N = len(y_pred)\n",
        "\n",
        "results = {}\n",
        "no_expl_results = {\n",
        "      'overall': [],\n",
        "      1: [],\n",
        "      0: [],\n",
        "      'gap': []\n",
        "  }\n",
        "for f_diff in fidelity_diff.range:\n",
        "  results[f_diff] = {\n",
        "      'overall': [],\n",
        "      1: [],\n",
        "      0: [],\n",
        "      'gap': []\n",
        "  }\n",
        "  fidelities = {\n",
        "      1: fidelity_mean.val + f_diff, # group 1 has higher fidelity than group 0\n",
        "      0: fidelity_mean.val - f_diff\n",
        "  }\n",
        "  for n in range(n_iters):\n",
        "    correct = y_pred == y_test\n",
        "    expl_random = np.random.random(size = (N,))\n",
        "    good_expl = np.zeros(shape = (N,))\n",
        "    for g in fidelities:\n",
        "      good_expl[g_test == g] = expl_random[g_test == g] <= fidelities[g]\n",
        "    user_correct = np.zeros(shape = (N,))\n",
        "    for expl in [0, 1]:\n",
        "      for bb in [0, 1]:\n",
        "        mask = (good_expl == expl) & (correct == bb)\n",
        "        user_correct[mask] = np.random.random(size = (mask.sum())) <= p_correct[bb][expl + 1].val \n",
        "    results[f_diff]['overall'].append(get_metric(metric, user_correct, y_test))\n",
        "    for g in fidelities:\n",
        "      mask = g_test == g\n",
        "      results[f_diff][g].append(get_metric(metric, user_correct[mask], y_test[mask]))\n",
        "    results[f_diff]['gap'].append(results[f_diff][1][-1] - results[f_diff][0][-1])\n",
        "\n",
        "    if f_diff == fidelity_diff.range[-1]:\n",
        "      user_correct = np.zeros(shape = (N,)) # no explanation\n",
        "      for bb in [0, 1]:\n",
        "        mask = (correct == bb)\n",
        "        user_correct[mask] = np.random.random(size = (mask.sum())) <= p_correct[bb][0].val \n",
        "      no_expl_results['overall'].append(get_metric(metric, user_correct, y_test))\n",
        "      for g in fidelities:\n",
        "        mask = g_test == g\n",
        "        no_expl_results[g].append(get_metric(metric, user_correct[mask], y_test[mask]))\n",
        "      no_expl_results['gap'].append(no_expl_results[1][-1] - no_expl_results[0][-1])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLuI6Dz9fNdz"
      },
      "outputs": [],
      "source": [
        "res_df = pd.DataFrame.from_dict(results, orient=\"index\").stack().to_frame()\n",
        "res_df = pd.DataFrame(res_df[0].values.tolist(), index=res_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trReAYRIg0s2"
      },
      "outputs": [],
      "source": [
        "means = res_df.mean(axis = 1).to_frame()\n",
        "cis = (1.96*res_df.std(axis = 1)/np.sqrt(n_iters)).to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEYmegUC41O0"
      },
      "outputs": [],
      "source": [
        "def get_cis(series):\n",
        "  avg = series.mean()\n",
        "  ci = 1.96*series.std()/np.sqrt(len(series))\n",
        "  return avg, avg - ci, avg + ci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5DCoyo4-hbi"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.plot(fidelity_diff.range, means.loc[pd.IndexSlice[:, 1], :][0], '-', label = 'Male', color = 'C0')\n",
        "ax.fill_between(fidelity_diff.range,  means.loc[pd.IndexSlice[:, 1], :][0] - cis.loc[pd.IndexSlice[:, 1], :][0],\n",
        "                means.loc[pd.IndexSlice[:, 1], :][0] + cis.loc[pd.IndexSlice[:, 1], :][0], alpha = 0.1, color = 'C0')\n",
        "\n",
        "ax.plot(fidelity_diff.range, means.loc[pd.IndexSlice[:, 0], :][0], '-', label = 'Female', color = 'C1')\n",
        "ax.fill_between(fidelity_diff.range,  means.loc[pd.IndexSlice[:, 0], :][0] - cis.loc[pd.IndexSlice[:, 0], :][0],\n",
        "                means.loc[pd.IndexSlice[:, 0], :][0] + cis.loc[pd.IndexSlice[:, 0], :][0], alpha = 0.1, color = 'C1')\n",
        "\n",
        "ax.set_xlabel('Explanation Fidelity Gap')\n",
        "if metric == 'accuracy':\n",
        "  ax.set_ylabel('Decision Accuracy')\n",
        "elif metric == 'F1':\n",
        "  ax.set_ylabel('Decision F1 Score')\n",
        "ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n",
        "ax.set_xlim([0, max(fidelity_diff.range)])\n",
        "ax.set_ylim([0.9, 0.92])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig.savefig('expl_gap_sim_nn.pdf', dpi = 300, bbox_inches = 'tight')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "simulate_explanation_impact_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
